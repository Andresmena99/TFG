{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Generación datos para LSTM\n",
    "\n",
    "En este notebook se llevará a cabo la implementación para generar los conjuntos de datos de entrada necesarios para entrenar la red LSTM. La implementación del entrenamiento de la propia red se puede encontrar en el notebook X."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from paciente import Paciente\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data, n_input: int, n_out: int, n_margin: int=1, drop_extremes_percentaje:float=0.0):\n",
    "\t\"\"\"Convertir los datos a un problema supervisado\n",
    "\n",
    "\tSe transforma el conjunto de datos a un dataset con entradas - salidas.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata (list):  \n",
    "\t\t\tLista con todos los datos de la serie que se quiere transformar.\n",
    "\n",
    "\t\tn_input (int): \n",
    "\t\t\tNumero de atributos (entradas) de cada muestra a generar.  \n",
    "\n",
    "\t\tn_out (int): \n",
    "\t\t\tNumero de salias de cada muestra a generar. \n",
    "\n",
    "\t\tn_margin (int): \n",
    "\t\t\tMargen entre una muestra y la siguiente que se genera.\n",
    "\t\t\n",
    "\t\tdrop_extremes_percentaje (float):\n",
    "\t\t\tPorcentaje del conjunto de datos que se quiere eliminar de los extremos\n",
    "\t\t\tdel conjunto de datos. 0.1 indica que se quiere eliminar un 10% del\n",
    "\t\t\tinicio de la muestra, y un 10% del final de la muestra.\n",
    "\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: \n",
    "\t\t\tEl primer elemento corresponde con las entradas (X) del conjunto\n",
    "\t\t\tde datos y el segundo con las salidas (y).\n",
    "\t\"\"\"\n",
    "\n",
    "\tif drop_extremes_percentaje > 0 and drop_extremes_percentaje < 0.5: # Elimino los extremos de la muestra.\n",
    "\t\tif drop_extremes_percentaje > 0.5:\n",
    "\t\t\tprint(f\"Error. El máximo valor de {drop_extremes_percentaje} es 0.5\")\n",
    "\t\telse:\n",
    "\t\t\tdata = data[int(len(data)*drop_extremes_percentaje):-int(len(data)*drop_extremes_percentaje)]\n",
    "\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# Definimos comienzo y fin de la muestra de entrada y de salida\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t\n",
    "\t\t# Comprobamos que tenemos datos suficientes\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tx_input = data[in_start:in_end]\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end])\n",
    "\n",
    "\t\t# Nos movemos tantos pasos como nos digan en n_margin\n",
    "\t\tin_start += n_margin\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_dataset(smooth_seconds = 5, seconds_prediction = 10, seconds_prior = 30, seconds_margin=1, nombres_ficheros=[\"data_3.csv\", \"data_5.csv\", \"data_7.csv\", \"data_8.csv\"], variables_significativas=['spo2'], drop_extremes_percentaje=0.1):\n",
    "\t\"\"\"Función que extrae todos los datos de un conjunto de pacientes\n",
    "\n",
    "\tA partir de la lista de pacientes, y considerando las características del\n",
    "    conjunto de datos especificadas en los parámetros, se generan los mismos.\n",
    "\n",
    "\tArgs:\n",
    "\t\tsmooth_seconds (int):  \n",
    "\t\t\tSegundos con los que se aplica la ventana deslizante para realizar el\n",
    "            suavizado de la señal.\n",
    "\n",
    "\t\tseconds_prediction (int): \n",
    "\t\t\tNumero de segundos posteriores (salidas) de cada muestra a generar. \n",
    "\n",
    "\t\tseconds_prior (int): \n",
    "\t\t\tNumero de segundos previos (entradas) de cada muestra a generar.  \n",
    "\n",
    "\t\tseconds_margin (int): \n",
    "\t\t\tMargen entre una muestra y la siguiente que se genera.\n",
    "\t\t\n",
    "        nombres_ficheros (list):\n",
    "            Lista de ficheros de pacientes con los que se quiere generar el\n",
    "            conjunto de datos.\n",
    "        \n",
    "        variables_significativas (list):\n",
    "            Lista de variables significativas que se consideran. Esta lista\n",
    "            define el número de señales que se usan para generar datos.\n",
    "\n",
    "\t\tdrop_extremes_percentaje (float):\n",
    "\t\t\tPorcentaje del conjunto de datos que se quiere eliminar de los extremos\n",
    "\t\t\tdel conjunto de datos. 0.1 indica que se quiere eliminar un 10% del\n",
    "\t\t\tinicio de la muestra, y un 10% del final de la muestra.\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: \n",
    "\t\t\tLista de listas con todo el conjunto de datos (cada una de las sublistas\n",
    "            corresponde con uno de los pacientes).\n",
    "\t\"\"\"\n",
    "    # Indicar la ruta donde se encuentra el fichero que se va a estudiar\n",
    "    all_data = np.empty(shape=(len(nombres_ficheros), 3), dtype=object)\n",
    "    dataset_dir = \"../data/pacientes\"\n",
    "\n",
    "    X, y = None, None\n",
    "    all_data = []\n",
    "\n",
    "    # Leemos cada fichero, y lo añadimos al conjunto de datos\n",
    "    for i, fichero in enumerate(nombres_ficheros):\n",
    "        # Indicamos las variables que vamos a querer estudiar. Tienen que ser variables que se encuentren disponibles en el fichero csv\n",
    "        paciente = Paciente(filename=os.path.join(dataset_dir, fichero), variables_significativas=variables_significativas)\n",
    "        paciente.comprobar_validez_dataset()\n",
    "\n",
    "        paciente.suavizar_seniales(smooth_seconds=smooth_seconds)\n",
    "\n",
    "        # Configuramos el tamaño de entrada y salida de las muestras de la red neuronal\n",
    "        time_step = paciente.time_step\n",
    "        n_out = round(seconds_prediction/time_step)\n",
    "\n",
    "        n_input = round(seconds_prior/time_step)\n",
    "\n",
    "        n_margin = round(seconds_margin/time_step)\n",
    "\n",
    "        # Añado al conjunto de datos toda la información de las variables significativas\n",
    "        data = [paciente.df_smothed[variable_significativa].astype(int) for variable_significativa in paciente.variables_significativas]\n",
    "        all_data.append(data)\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2):\n",
    "    \"\"\"División en conjuntos de entrenamiento, test y validación\n",
    "\n",
    "    Args:\n",
    "        X (list):  \n",
    "            Lista con los valores de entrada de cada muestra que se desean \n",
    "            dividir en conjuntos de entrenamiento.\n",
    "\n",
    "        y (list):  \n",
    "            Lista con los valores de salida de cada muestra que se desean \n",
    "            dividir en conjuntos de entrenamiento.\n",
    "\n",
    "        train_size (float): \n",
    "            Porcentaje de datos destinados al conjunto de entrenamiento.  \n",
    "\n",
    "        val_size (float): \n",
    "            Porcentaje de datos destinados al conjunto de validacion.  \n",
    "\n",
    "        test_size (float): \n",
    "            Porcentaje de datos destinados al conjunto de test. \n",
    "\n",
    "    Returns:\n",
    "        list: \n",
    "            Lista donde cada uno de los elementos corresponde con las \n",
    "            siguientes variables (autoexplicativas): X_train, y_train, X_test, \n",
    "            y_test, X_val, y_val.\n",
    "    \"\"\"\n",
    "\n",
    "    if train_size + val_size + test_size != 1:\n",
    "        print(\"Error, parametros incorrectos\")\n",
    "        return None, None, None\n",
    "\n",
    "    num_elements_train = int(len(X)* train_size)\n",
    "    num_elements_test = int(len(X)* test_size)\n",
    "\n",
    "    return X[:num_elements_train], y[:num_elements_train], X[num_elements_train:num_elements_train+num_elements_test], y[num_elements_train:num_elements_train+num_elements_test], X[num_elements_train+num_elements_test:], y[num_elements_train+num_elements_test:]"
   ]
  },
  {
   "source": [
    "## Configuración de variables\n",
    "\n",
    "En esta sección, se debe indicar la ruta donde se encuentra la información de los pacientes que se desean utilizar para generar el conjunto de datos que utilizará la red LSTM. De igual forma, se deben indicar algunos parámetros para la generación de los datos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../data/pacientes\"\n",
    "lstm_dir = \"lstm\"\n",
    "nombres_ficheros = [\"data_3.csv\", \"data_5.csv\", \"data_7.csv\", \"data_8.csv\"]\n",
    "\n",
    "seconds_prediction = 10\n",
    "seconds_prior = 30\n",
    "seconds_margin=1\n",
    "time_step = 1/3\n",
    "\n",
    "n_out = round(seconds_prediction/time_step)\n",
    "n_input = round(seconds_prior/time_step)\n",
    "n_margin = round(seconds_margin/time_step)\n",
    "\n",
    "drop_extremes_percentaje = 0.1\n",
    "\n",
    "train_size=0.6\n",
    "val_size=0.2\n",
    "test_size=0.2"
   ]
  },
  {
   "source": [
    "## Generacion dataset univariable\n",
    "\n",
    "A continuación, se generará el dataset con los parámetros previamente indicados, usando únicamente la señal de saturación como entrada al modelo, y siendo la salida del modelo la señal de saturación.\n",
    "\n",
    "Cabe destacar que se realiza el escalado de los datos en el rango -1, 1, óptimo para usar la función ReLu como salida de la red LSTM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = generar_dataset(smooth_seconds=0, seconds_prediction = seconds_prediction, seconds_prior = seconds_prior, seconds_margin = seconds_margin, variables_significativas=[\"spo2\"], nombres_ficheros=nombres_ficheros, drop_extremes_percentaje=drop_extremes_percentaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos el conjunto de datos que se van a usar para train, para utilizar estos datos para realizar el escalado del dataset\n",
    "train_data_spo2 = [current_data[:int(len(current_data[0])*train_size)] for current_data in all_data]\n",
    "train_data_spo2 = np.concatenate(train_data_spo2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación, escalamos nuestros datos en el rango -1, 1\n",
    "scaler_spo2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_spo2.fit(np.reshape(train_data_spo2, (-1, 1)))\n",
    "all_data_min_max_spo2 = [scaler_spo2.transform(np.reshape(current_data[0].values, (-1, 1))) for current_data in all_data]"
   ]
  },
  {
   "source": [
    "Una vez tenemos todos los datos escalados correctamente, generamos nuestros conjuntos de entrenamiento, validacion y test completos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = None \n",
    "y_train = None\n",
    "x_test = None\n",
    "y_test = None\n",
    "x_val = None\n",
    "y_val = None\n",
    "\n",
    "for current_data in all_data_min_max_spo2:\n",
    "    current_x, current_y = to_supervised(current_data.ravel(), n_input=n_input, n_out=n_out, n_margin=n_margin, drop_extremes_percentaje=drop_extremes_percentaje)\n",
    "\n",
    "    curr_x_train, curr_y_train, curr_x_test, curr_y_test, curr_x_val, curr_y_val = train_test_val_split(current_x, current_y, train_size=train_size, val_size=val_size, test_size=test_size)\n",
    "\n",
    "    if x_train is None: x_train = curr_x_train\n",
    "    else: x_train = np.concatenate((x_train, curr_x_train))\n",
    "\n",
    "    if y_train is None: y_train = curr_y_train\n",
    "    else: y_train = np.concatenate((y_train, curr_y_train))\n",
    "\n",
    "    if x_test is None: x_test = curr_x_test\n",
    "    else: x_test = np.concatenate((x_test, curr_x_test))\n",
    "    \n",
    "    if y_test is None: y_test = curr_y_test\n",
    "    else: y_test = np.concatenate((y_test, curr_y_test))\n",
    "        \n",
    "    if x_val is None: x_val = curr_x_val\n",
    "    else: x_val = np.concatenate((x_val, curr_x_val))\n",
    "    \n",
    "    if y_val is None: y_val = curr_y_val\n",
    "    else: y_val = np.concatenate((y_val, curr_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "x_val = x_val.reshape((x_val.shape[0], x_val.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((49822, 90, 1),\n",
       " (49822, 30),\n",
       " (16606, 90, 1),\n",
       " (16606, 30),\n",
       " (16611, 90, 1),\n",
       " (16611, 30))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "source": [
    "Podemos ver en la celda anterior el tamaño de los conjuntos de entrenamiento, test y validación respectivamente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{lstm_dir}/dataset_univariable.pickle\", \"wb\") as handle:\n",
    "    pickle.dump([x_train, y_train, x_test, y_test, x_val, y_val, scaler_spo2], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "## Generacion dataset multivariable\n",
    "\n",
    "A continuación, se generará el dataset con los parámetros previamente indicados, usando las señales de pulso y saturación como entrada al modelo, y siendo la salida del modelo la señal de saturación.\n",
    "\n",
    "Cabe destacar que se realiza el escalado de los datos en el rango -1, 1, óptimo para usar la función ReLu como salida de la red LSTM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = generar_dataset(smooth_seconds=0, seconds_prediction = seconds_prediction, seconds_prior = seconds_prior, seconds_margin = seconds_margin, variables_significativas=[\"spo2\", \"pulso\"], nombres_ficheros=nombres_ficheros, drop_extremes_percentaje=drop_extremes_percentaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos el conjunto de datos que se van a usar para train, tanto de la señal del pulso como\n",
    "# la saturación, para utilizar estos datos para realizar el escalado del dataset\n",
    "train_data_spo2 = [current_data[0][:int(len(current_data[0])*train_size)] for current_data in all_data]\n",
    "train_data_spo2 = np.concatenate(train_data_spo2)\n",
    "\n",
    "train_data_pulso = [current_data[1][:int(len(current_data[1])*train_size)] for current_data in all_data]\n",
    "train_data_pulso = np.concatenate(train_data_pulso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación, escalamos nuestros datos en el rango -1, 1\n",
    "scaler_spo2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_spo2.fit(np.reshape(train_data_spo2, (-1, 1)))\n",
    "all_data_min_max_spo2 = [scaler_spo2.transform(np.reshape(current_data[0].values, (-1, 1))) for current_data in all_data]\n",
    "\n",
    "scaler_pulso = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_pulso.fit(np.reshape(train_data_pulso, (-1, 1)))\n",
    "all_data_min_max_pulso = [scaler_pulso.transform(np.reshape(current_data[1].values, (-1, 1))) for current_data in all_data]"
   ]
  },
  {
   "source": [
    "Una vez tenemos todos los datos escalados correctamente, generamos nuestros conjuntos de entrenamiento, validacion y test completos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [None, None]\n",
    "y_train = None\n",
    "x_test = [None, None]\n",
    "y_test = None\n",
    "x_val = [None, None]\n",
    "y_val = None\n",
    "\n",
    "for data_spo2_pulso in zip(all_data_min_max_spo2, all_data_min_max_pulso):\n",
    "    for i, current_data in enumerate(data_spo2_pulso):\n",
    "\n",
    "        current_x, current_y = to_supervised(current_data.ravel(), n_input=n_input, n_out=n_out, n_margin=n_margin, drop_extremes_percentaje=drop_extremes_percentaje)\n",
    "\n",
    "        curr_x_train, curr_y_train, curr_x_test, curr_y_test, curr_x_val, curr_y_val = train_test_val_split(current_x, current_y, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "\n",
    "        if x_train[i] is None: x_train[i] = curr_x_train\n",
    "        else: x_train[i] = np.concatenate((x_train[i], curr_x_train))\n",
    "\n",
    "        if i == 0: # La salida es univariada\n",
    "            if y_train is None: y_train = curr_y_train\n",
    "            else: y_train = np.concatenate((y_train, curr_y_train))\n",
    "\n",
    "        if x_test[i] is None: x_test[i] = curr_x_test\n",
    "        else: x_test[i] = np.concatenate((x_test[i], curr_x_test))\n",
    "        \n",
    "        if i == 0: # La salida es univariada\n",
    "            if y_test is None: y_test = curr_y_test\n",
    "            else: y_test = np.concatenate((y_test, curr_y_test))\n",
    "            \n",
    "        if x_val[i] is None: x_val[i] = curr_x_val\n",
    "        else: x_val[i] = np.concatenate((x_val[i], curr_x_val))\n",
    "        \n",
    "        if i == 0: # La salida es univariada\n",
    "            if y_val is None: y_val = curr_y_val\n",
    "            else: y_val = np.concatenate((y_val, curr_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.stack(x_train, axis=2)\n",
    "x_test = np.stack(x_test, axis=2)\n",
    "x_val = np.stack(x_val, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((49822, 90, 2),\n",
       " (49822, 30),\n",
       " (16606, 90, 2),\n",
       " (16606, 30),\n",
       " (16611, 90, 2),\n",
       " (16611, 30))"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "source": [
    "Podemos ver en la celda anterior el tamaño de los conjuntos de entrenamiento, test y validación respectivamente.\n",
    "\n",
    "Como podemos observar, la última dimensión de la columna de los datos de entrada al modelo, ha pasado de ser 1 a 2, ya que ahora usamos como entrada no solo la señal de saturación sino también la del pulso"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{lstm_dir}/dataset_multivariable.pickle\", \"wb\") as handle:\n",
    "    pickle.dump([x_train, y_train, x_test, y_test, x_val, y_val, scaler_spo2], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ]
}